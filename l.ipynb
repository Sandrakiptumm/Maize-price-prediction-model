{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_categorical_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     load_and_combine_data,\n\u001b[0;32m      9\u001b[0m     clean_columns,\n\u001b[0;32m     10\u001b[0m     replace_missing_values,\n\u001b[0;32m     11\u001b[0m     convert_price_columns,\n\u001b[0;32m     12\u001b[0m     impute_missing_values,\n\u001b[0;32m     13\u001b[0m     filter_markets,\n\u001b[0;32m     14\u001b[0m     remove_outliers,\n\u001b[0;32m     15\u001b[0m     export_data,\n\u001b[0;32m     16\u001b[0m     feature_engineering_pipeline,\n\u001b[0;32m     17\u001b[0m     train_lstm_model,\n\u001b[0;32m     18\u001b[0m      encode_categorical_feature)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_and_save_all\n",
      "File \u001b[1;32mc:\\Users\\Hp\\Documents\\fifth year\\maize 3\\functions.py:250\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature engineering and export complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfeature_engineering_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_lstm_model\u001b[39m(\n\u001b[0;32m    254\u001b[0m     target_variable, \n\u001b[0;32m    255\u001b[0m     scaler_feature_path, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m ):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodeling_data_2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Hp\\Documents\\fifth year\\maize 3\\functions.py:223\u001b[0m, in \u001b[0;36mfeature_engineering_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m data \u001b[38;5;241m=\u001b[39m add_rolling_features(data, rolling_windows\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m7d\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m7\u001b[39m})\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Encode and filter features\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mencode_categorical_features\u001b[49m(data, columns_to_encode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarket\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassification\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    224\u001b[0m target_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetail\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWholesale\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    225\u001b[0m filtered_column_names \u001b[38;5;241m=\u001b[39m filter_columns_by_correlation(data, target_columns, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encode_categorical_features' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from maize_data_scrapper import scrape_data\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from functions import (\n",
    "    load_and_combine_data,\n",
    "    clean_columns,\n",
    "    replace_missing_values,\n",
    "    convert_price_columns,\n",
    "    impute_missing_values,\n",
    "    filter_markets,\n",
    "    remove_outliers,\n",
    "    export_data,\n",
    "    feature_engineering_pipeline,\n",
    "    train_lstm_model)\n",
    "from predict import predict_and_save_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data = load_and_combine_data(file_paths, scraped_csv_path=scraped_csv_path)\n",
    "    data = clean_columns(data)\n",
    "    data = replace_missing_values(data)\n",
    "    data = convert_price_columns(data, price_columns)\n",
    "    data = impute_missing_values(data, knn_columns)\n",
    "    data = data.dropna()\n",
    "    data.sort_values(by=['County', 'Market', 'Classification', 'Date'], inplace=True)\n",
    "    data = filter_markets(data, threshold=10)\n",
    "    data = remove_outliers(data, num_columns)\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Save cleaned data\n",
    "    clean_data_path = \"clean_data2.csv\"\n",
    "    export_data(data, file_name=clean_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_pipeline(\n",
    "    scraped_csv_path=\"maize_data.csv\",\n",
    "    file_paths=[ \n",
    "        \"raw data/Market Prices.xls\", \"raw data/Market Prices 2.xls\",\n",
    "        \"raw data/Market Prices 3.xls\", \"raw data/Market Prices 4.xls\",\n",
    "        \"raw data/Market Prices 5.xls\", \"raw data/Market Prices 6.xls\",\n",
    "        \"raw data/Market Prices 7.xls\", \"raw data/Market Prices 8.xls\"\n",
    "    ],\n",
    "    price_columns=[\"Wholesale\", \"Retail\"],\n",
    "    knn_columns=[\"Supply Volume\", \"Retail\", \"Wholesale\"],\n",
    "    num_columns=[\"Retail\", \"Wholesale\", \"Supply Volume\"],\n",
    "    forecast_horizon=20,\n",
    "    counties=['Baringo', 'Bomet', 'Bungoma', 'Busia', 'Elgeyo-Marakwet', 'Embu',\n",
    "       'Garissa', 'Homa-bay', 'Isiolo', 'Kajiado', 'Kakamega', 'Kericho',\n",
    "       'Kiambu', 'Kilifi', 'Kirinyaga', 'Kisii', 'Kisumu', 'Kitui',\n",
    "       'Kwale', 'Laikipia', 'Lamu', 'Machakos', 'Makueni', 'Mandera',\n",
    "       'Meru', 'Migori', 'Mombasa', 'Muranga', 'Nairobi', 'Nakuru',\n",
    "       'Nandi', 'Narok', 'Nyamira', 'Nyandarua', 'Nyeri', 'Samburu',\n",
    "       'Siaya', 'Taita-Taveta', 'Tana-River', 'Tharaka-Nithi',\n",
    "       'Trans-Nzoia', 'Turkana', 'Uasin-Gishu', 'Vihiga', 'Wajir',\n",
    "       'West-Pokot'],\n",
    "    prediction_save_path=\"predictions.csv\"\n",
    "):\n",
    "    # Step 1: Scrape data\n",
    "    print(\"Step 1: Scraping data...\")\n",
    "    scrape_data()\n",
    "\n",
    "    # Step 2: Combine and clean data\n",
    "    print(\"Step 2: Cleaning data...\")\n",
    "    data = load_and_combine_data(file_paths, scraped_csv_path=scraped_csv_path)\n",
    "    data = clean_columns(data)\n",
    "    data = replace_missing_values(data)\n",
    "    data = convert_price_columns(data, price_columns)\n",
    "    data = impute_missing_values(data, knn_columns)\n",
    "    data = data.dropna()\n",
    "    data.sort_values(by=['County', 'Market', 'Classification', 'Date'], inplace=True)\n",
    "    data = filter_markets(data, threshold=10)\n",
    "    data = remove_outliers(data, num_columns)\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Save cleaned data\n",
    "    clean_data_path = \"clean_data2.csv\"\n",
    "    export_data(data, file_name=clean_data_path)\n",
    "\n",
    "    # Step 3: Perform feature engineering\n",
    "    print(\"Step 3: Feature engineering...\")\n",
    "    feature_engineering_pipeline()\n",
    "\n",
    "    # Step 4: Retrain models\n",
    "    print(\"Step 4: Retraining models...\")\n",
    "\n",
    "    # Wholesale model\n",
    "    train_lstm_model(\n",
    "        target_variable='Wholesale',\n",
    "        scaler_feature_path=\"models/scaler_wholesale_features.pkl\",\n",
    "        scaler_target_path=\"models/scaler_wholesale_target.pkl\",\n",
    "        model_save_path=\"models/best_wholesale_model_sequence.h5\"\n",
    "    )\n",
    "    # Retail model\n",
    "    train_lstm_model(\n",
    "        target_variable='Retail',\n",
    "        scaler_feature_path=\"models/scaler_retail_features.pkl\",\n",
    "        scaler_target_path=\"models/scaler_retail_target.pkl\",\n",
    "        model_save_path=\"models/best_retail_model_sequence.h5\"\n",
    "    )\n",
    "\n",
    "    # Step 5: Generate predictions using the new predict_and_save function\n",
    "    print(\"Step 5: Generating predictions...\")\n",
    "\n",
    "    predict_and_save_all(\n",
    "    current_date=\"2024-12-11\",\n",
    "    reference_data=pd.read_csv(\"historical_data.csv\"),\n",
    "    wholesale_model=tf.keras.models.load_model(r\"models/best_wholesale_model_sequence.h5\"),\n",
    "    retail_model=tf.keras.models.load_model(r\"models/best_retail_model_sequence.h5\"),\n",
    "    wholesale_scalers=(joblib.load(\"models/scaler_wholesale_features.pkl\"), joblib.load(\"models/scaler_wholesale_target.pkl\")),\n",
    "    retail_scalers=(joblib.load(\"models/scaler_retail_features.pkl\"), joblib.load(\"models/scaler_retail_target.pkl\")),\n",
    "    encoder=joblib.load(\"models/County_binary_encoder.pkl\"),\n",
    "    counties=[\n",
    "    'Baringo', 'Bomet', 'Bungoma', 'Busia', 'Elgeyo-Marakwet', 'Embu',\n",
    "    'Garissa', 'Homa-bay', 'Isiolo', 'Kajiado', 'Kakamega', 'Kericho',\n",
    "    'Kiambu', 'Kilifi', 'Kirinyaga', 'Kisii', 'Kisumu', 'Kitui',\n",
    "    'Kwale', 'Laikipia', 'Lamu', 'Machakos', 'Makueni', 'Mandera',\n",
    "    'Meru', 'Migori', 'Mombasa', 'Muranga', 'Nairobi', 'Nakuru',\n",
    "    'Nandi', 'Narok', 'Nyamira', 'Nyandarua', 'Nyeri', 'Samburu',\n",
    "    'Siaya', 'Taita-Taveta', 'Tana-River', 'Tharaka-Nithi',\n",
    "    'Trans-Nzoia', 'Turkana', 'Uasin-Gishu', 'Vihiga', 'Wajir',\n",
    "    'West-Pokot'\n",
    "    ],\n",
    "    output_file=\"predictions.csv\")\n",
    "\n",
    "    print(\"Automation complete! Predictions saved for all counties and models retrained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
