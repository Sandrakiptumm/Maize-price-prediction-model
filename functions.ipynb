{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy import stats\n",
    "from flask import Flask, jsonify, request\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from scipy import stats\n",
    "import os\n",
    "from flask import request, jsonify\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw data/Market Prices.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Run the main function if this script is executed\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m num_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWholesale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupply Volume\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Process the data\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_combine_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m data \u001b[38;5;241m=\u001b[39m clean_columns(data)\n\u001b[0;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m replace_missing_values(data)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mload_and_combine_data\u001b[1;34m(file_paths)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_combine_data\u001b[39m(file_paths):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load multiple Excel files and combine them into a single DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_excel(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_combine_data\u001b[39m(file_paths):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load multiple Excel files and combine them into a single DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\env_2\\lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\env_2\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1496\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1496\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1501\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1502\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1503\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\env_2\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1371\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1369\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1374\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1375\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\env_2\\lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw data/Market Prices.xls'"
     ]
    }
   ],
   "source": [
    "def load_and_combine_data(file_paths):\n",
    "    \"\"\"Load multiple Excel files and combine them into a single DataFrame.\"\"\"\n",
    "    dfs = [pd.read_excel(file) for file in file_paths]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def clean_columns(data):\n",
    "    \"\"\"Drop irrelevant columns.\"\"\"\n",
    "    return data.drop(['Commodity', 'Grade', 'Sex'], axis=1)\n",
    "\n",
    "def replace_missing_values(data):\n",
    "    \"\"\"Replace hyphens with NaN to handle missing values.\"\"\"\n",
    "    data.replace(['-', ' - ', '- ', ' -'], np.nan, inplace=True)\n",
    "    return data\n",
    "\n",
    "def convert_price_columns(data, price_columns):\n",
    "    \"\"\"Convert price columns to numerical values.\"\"\"\n",
    "    for col in price_columns:\n",
    "        data[col] = data[col].str.lower().str.replace(\"/kg\", \"\").str.strip()\n",
    "        data[col] = data[col].str.replace(\"s\", \"\").str.strip().astype(float)\n",
    "    return data\n",
    "\n",
    "def impute_missing_values(data, columns, n_neighbors=5):\n",
    "    \"\"\"Use KNN imputer to fill in missing values.\"\"\"\n",
    "    knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    data[columns] = knn_imputer.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "def filter_markets(data, threshold=10):\n",
    "    \"\"\"Filter out markets with less than a specified number of records.\"\"\"\n",
    "    market_counts = data[\"Market\"].value_counts()\n",
    "    markets_to_keep = market_counts[market_counts >= threshold].index\n",
    "    return data[data['Market'].isin(markets_to_keep)]\n",
    "\n",
    "def remove_outliers(data, columns):\n",
    "    \"\"\"Remove rows with outliers based on z-score thresholding.\"\"\"\n",
    "    outliers = np.zeros(data.shape[0], dtype=bool)\n",
    "    for col in columns:\n",
    "        z_scores = stats.zscore(data[col].dropna())\n",
    "        outliers = outliers | (np.abs(z_scores) > 3)\n",
    "    return data[~outliers]\n",
    "\n",
    "def export_data(data, file_name=\"clean_data.csv\"):\n",
    "    \"\"\"Export cleaned data to a CSV file.\"\"\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "\n",
    "def main():\n",
    "    # Define file paths and columns\n",
    "    file_paths = [\n",
    "        \"raw data/Market Prices.xls\", \"raw data/Market Prices 2.xls\",\n",
    "        \"raw data/Market Prices 3.xls\", \"raw data/Market Prices 4.xls\",\n",
    "        \"raw data/Market Prices 5.xls\", \"Raw Data/Market Prices 6.xls\",\n",
    "        \"raw data/Market Prices 7.xls\", \"Raw Data/Market Prices 8.xls\"\n",
    "    ]\n",
    "    price_columns = [\"Wholesale\", \"Retail\"]\n",
    "    knn_columns = [\"Supply Volume\", \"Retail\", \"Wholesale\"]\n",
    "    num_columns = [\"Retail\", \"Wholesale\", \"Supply Volume\"]\n",
    "\n",
    "    # Process the data\n",
    "    data = load_and_combine_data(file_paths)\n",
    "    data = clean_columns(data)\n",
    "    data = replace_missing_values(data)\n",
    "    data = convert_price_columns(data, price_columns)\n",
    "    data = impute_missing_values(data, knn_columns)\n",
    "    data = data.dropna()\n",
    "    data.sort_values(by=['County', 'Market', 'Classification', 'Date'], inplace=True)\n",
    "    data = filter_markets(data, threshold=10)\n",
    "    data = remove_outliers(data, num_columns)\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Export cleaned data\n",
    "    export_data(data, \"clean_data2.csv\")\n",
    "    print(\"Data cleaning and export complete.\")\n",
    "\n",
    "# Run the main function if this script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the cleaned CSV data.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_time_features(data):\n",
    "    \"\"\"Extract year, month, day, day of the week, and quarter from date.\"\"\"\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "    data['Quarter'] = data['Date'].dt.quarter\n",
    "    return data\n",
    "\n",
    "def add_cyclic_features(data):\n",
    "    \"\"\"Add cyclic features for month, day, day of the week, quarter, and year.\"\"\"\n",
    "    data['Month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n",
    "    data['Month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n",
    "    data['Day_sin'] = np.sin(2 * np.pi * data['Day'] / 31)\n",
    "    data['Day_cos'] = np.cos(2 * np.pi * data['Day'] / 31)\n",
    "    data['DayOfWeek_sin'] = np.sin(2 * np.pi * data['DayOfWeek'] / 7)\n",
    "    data['DayOfWeek_cos'] = np.cos(2 * np.pi * data['DayOfWeek'] / 7)\n",
    "    data['Quarter_sin'] = np.sin(2 * np.pi * data['Quarter'] / 4)\n",
    "    data['Quarter_cos'] = np.cos(2 * np.pi * data['Quarter'] / 4)\n",
    "    year_range = data['Year'].max() - data['Year'].min()\n",
    "    data['Year_sin'] = np.sin(2 * np.pi * (data['Year'] - data['Year'].min()) / year_range)\n",
    "    data['Year_cos'] = np.cos(2 * np.pi * (data['Year'] - data['Year'].min()) / year_range)\n",
    "    return data\n",
    "\n",
    "def add_lagged_features(data, lag_days=7):\n",
    "    \"\"\"Add lagged features for wholesale, retail, and supply volume.\"\"\"\n",
    "    for lag in [lag_days]: \n",
    "        data[f'Wholesale_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Wholesale'].shift(lag)\n",
    "        data[f'Retail_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Retail'].shift(lag)\n",
    "        data[f'Supply_Volume_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Supply Volume'].shift(lag)\n",
    "    return data\n",
    "\n",
    "def add_rolling_features(data, rolling_windows={'7d': 7}):\n",
    "    \"\"\"Add rolling mean and std features for wholesale, retail, and supply volume.\"\"\"\n",
    "    data = data.sort_values(by=['Market', 'Classification', 'Date'])\n",
    "    for window_name, window_size in rolling_windows.items():\n",
    "        for column in ['Wholesale', 'Retail', 'Supply Volume']:\n",
    "            data[f'{column}_rolling_mean_{window_name}'] = data.groupby(['Market', 'Classification'])[column].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "            data[f'{column}_rolling_std_{window_name}'] = data.groupby(['Market', 'Classification'])[column].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n",
    "    return data.bfill().ffill()\n",
    "\n",
    "def encode_categorical_features(data, columns_to_encode):\n",
    "    \"\"\"Binary encode specified categorical columns.\"\"\"\n",
    "    binary_encoder = ce.BinaryEncoder(cols=columns_to_encode, return_df=True)\n",
    "    return binary_encoder.fit_transform(data)\n",
    "\n",
    "def filter_columns_by_correlation(data, target_columns, threshold=0.1):\n",
    "    \"\"\"Filter columns based on correlation with target columns.\"\"\"\n",
    "    correlation_matrix = data.corr()\n",
    "    correlation_with_target = correlation_matrix[target_columns]\n",
    "    filtered_columns = correlation_with_target[(correlation_with_target['Retail'].abs() > threshold) | \n",
    "                                               (correlation_with_target['Wholesale'].abs() > threshold)]\n",
    "    filtered_column_names = [col for col in filtered_columns.index if col not in target_columns]\n",
    "    return data[filtered_column_names + target_columns]\n",
    "\n",
    "def export_modeling_data(data, file_name=\"modeling_data.csv\"):\n",
    "    \"\"\"Export the final dataset for modeling.\"\"\"\n",
    "    data.to_csv(file_name, index=False)\n",
    "\n",
    "def feature_engineering_pipeline():\n",
    "    # Load and process data\n",
    "    data = load_data(\"clean_data2.csv\")\n",
    "    data = extract_time_features(data)\n",
    "    data = add_cyclic_features(data)\n",
    "    data = add_lagged_features(data)\n",
    "    data = add_rolling_features(data, rolling_windows={'7d': 7})\n",
    "    \n",
    "    # Encode and filter features\n",
    "    data = encode_categorical_features(data, columns_to_encode=['County', 'Market', 'Classification'])\n",
    "    final_data = filter_columns_by_correlation(data, target_columns=['Retail', 'Wholesale'], threshold=0.1)\n",
    "    \n",
    "    # Export data\n",
    "    export_modeling_data(final_data, \"modeling_data_2.csv\")\n",
    "    print(\"Feature engineering and export complete.\")\n",
    "\n",
    "# Run the feature engineering pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    feature_engineering_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Flask(__name__)\n",
    "\n",
    "# Step 1: Define each function for the pipeline\n",
    "\n",
    "def load_data(file_path=\"clean_data2.csv\"):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_time_features(data):\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    data['Month'] = data['Date'].dt.month\n",
    "    data['Day'] = data['Date'].dt.day\n",
    "    data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "    data['Quarter'] = data['Date'].dt.quarter\n",
    "    return data\n",
    "\n",
    "def add_cyclic_features(data):\n",
    "    data['Month_sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n",
    "    data['Month_cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n",
    "    data['Day_sin'] = np.sin(2 * np.pi * data['Day'] / 31)\n",
    "    data['Day_cos'] = np.cos(2 * np.pi * data['Day'] / 31)\n",
    "    data['DayOfWeek_sin'] = np.sin(2 * np.pi * data['DayOfWeek'] / 7)\n",
    "    data['DayOfWeek_cos'] = np.cos(2 * np.pi * data['DayOfWeek'] / 7)\n",
    "    data['Quarter_sin'] = np.sin(2 * np.pi * data['Quarter'] / 4)\n",
    "    data['Quarter_cos'] = np.cos(2 * np.pi * data['Quarter'] / 4)\n",
    "    year_range = data['Year'].max() - data['Year'].min()\n",
    "    data['Year_sin'] = np.sin(2 * np.pi * (data['Year'] - data['Year'].min()) / year_range)\n",
    "    data['Year_cos'] = np.cos(2 * np.pi * (data['Year'] - data['Year'].min()) / year_range)\n",
    "    return data\n",
    "\n",
    "def add_lagged_features(data, lag_days=7):\n",
    "    for lag in [lag_days]: \n",
    "        data[f'Wholesale_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Wholesale'].shift(lag)\n",
    "        data[f'Retail_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Retail'].shift(lag)\n",
    "        data[f'Supply_Volume_lag_{lag}'] = data.groupby(['County','Market', 'Classification'])['Supply Volume'].shift(lag)\n",
    "    return data\n",
    "\n",
    "def add_rolling_features(data, rolling_windows={'7d': 7}):\n",
    "    data = data.sort_values(by=['Market', 'Classification', 'Date'])\n",
    "    for window_name, window_size in rolling_windows.items():\n",
    "        for column in ['Wholesale', 'Retail', 'Supply Volume']:\n",
    "            data[f'{column}_rolling_mean_{window_name}'] = data.groupby(['Market', 'Classification'])[column].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "            data[f'{column}_rolling_std_{window_name}'] = data.groupby(['Market', 'Classification'])[column].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n",
    "    return data.bfill().ffill()\n",
    "\n",
    "def encode_categorical_features(data):\n",
    "    columns_to_encode = ['County', 'Market', 'Classification']\n",
    "    binary_encoder = ce.BinaryEncoder(cols=columns_to_encode, return_df=True)\n",
    "    return binary_encoder.fit_transform(data)\n",
    "\n",
    "def filter_columns_by_correlation(data, target_columns=['Retail', 'Wholesale'], threshold=0.1):\n",
    "    correlation_matrix = data.corr()\n",
    "    correlation_with_target = correlation_matrix[target_columns]\n",
    "    filtered_columns = correlation_with_target[(correlation_with_target.abs() > threshold).any(axis=1)]\n",
    "    filtered_column_names = filtered_columns.index.tolist()\n",
    "    filtered_column_names = [col for col in filtered_column_names if col not in target_columns]\n",
    "    final_data = data[filtered_column_names].copy()\n",
    "    final_data = final_data.join(data[target_columns])\n",
    "    return final_data\n",
    "\n",
    "def export_modeling_data(data, output_file=\"modeling_data_2.csv\"):\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Data exported to {output_file}\")\n",
    "\n",
    "# Step 2: Define the pipeline\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('load_data', FunctionTransformer(load_data)),\n",
    "    ('extract_time_features', FunctionTransformer(extract_time_features)),\n",
    "    ('add_cyclic_features', FunctionTransformer(add_cyclic_features)),\n",
    "    ('add_lagged_features', FunctionTransformer(add_lagged_features)),\n",
    "    ('add_rolling_features', FunctionTransformer(add_rolling_features)),\n",
    "    ('encode_categorical_features', FunctionTransformer(encode_categorical_features)),\n",
    "    ('filter_columns_by_correlation', FunctionTransformer(filter_columns_by_correlation)),\n",
    "    ('export_data', FunctionTransformer(export_modeling_data)),\n",
    "])\n",
    "\n",
    "# Step 3: API Endpoint to Trigger the Pipeline\n",
    "@app.route('/process_data', methods=['POST'])\n",
    "def process_data():\n",
    "    try:\n",
    "        # Run the pipeline\n",
    "        feature_engineering_pipeline.fit_transform(None)\n",
    "        \n",
    "        # Return success message\n",
    "        return jsonify({\"message\": \"Data processing completed and saved as 'modeling_data_2.csv'.\"}), 200\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions and return an error message\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = '../models/final_model.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Define data preparation functions\n",
    "def load_data(file_path):\n",
    "    # Load raw data and perform initial cleaning steps if needed\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_time_features(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    return df\n",
    "\n",
    "def add_cyclic_features(df):\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "    df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    df['Quarter_sin'] = np.sin(2 * np.pi * df['Quarter'] / 4)\n",
    "    df['Quarter_cos'] = np.cos(2 * np.pi * df['Quarter'] / 4)\n",
    "    return df\n",
    "\n",
    "def encode_categorical_features(df):\n",
    "    encoder = ce.BinaryEncoder(cols=['County', 'Market', 'Classification'], return_df=True)\n",
    "    return encoder.fit_transform(df)\n",
    "\n",
    "# Pipeline for data preparation\n",
    "data_preparation_pipeline = Pipeline([\n",
    "    ('load_data', FunctionTransformer(load_data)),\n",
    "    ('extract_time_features', FunctionTransformer(extract_time_features)),\n",
    "    ('add_cyclic_features', FunctionTransformer(add_cyclic_features)),\n",
    "    ('encode_categorical_features', FunctionTransformer(encode_categorical_features)),\n",
    "])\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get data from request\n",
    "        file_path = request.json.get('file_path')\n",
    "        \n",
    "        # Run data preparation pipeline\n",
    "        prepared_data = data_preparation_pipeline.fit_transform(file_path)\n",
    "        \n",
    "        # Select the last 7 days of features for prediction\n",
    "        n_timesteps = 7\n",
    "        if len(prepared_data) < n_timesteps:\n",
    "            return jsonify({\"error\": \"Not enough data for the specified time steps\"}), 400\n",
    "        prediction_data = prepared_data[-n_timesteps:].values.reshape((1, n_timesteps, prepared_data.shape[1]))\n",
    "\n",
    "        # Make predictions using the loaded model\n",
    "        prediction = model.predict(prediction_data)\n",
    "        \n",
    "        return jsonify({\"prediction\": prediction[0][0]}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "MODEL_PATH = '../models/final_model.h5'\n",
    "\n",
    "# Helper function to prepare data from request\n",
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    Converts input data from JSON format to a numpy array for the model.\n",
    "    Expects a list of lists, where each inner list represents a feature set at a specific timestep.\n",
    "    \"\"\"\n",
    "    return np.array(data).reshape(1, -1, len(data[0]))  # Reshape for LSTM (1, timesteps, features)\n",
    "\n",
    "\n",
    "@app.route('/retrain', methods=['POST'])\n",
    "def retrain_lstm_model():\n",
    "    \"\"\"\n",
    "    Retrains the saved model with new training data.\n",
    "    Expects JSON input with 'train_X', 'train_y', 'validation_X', and 'validation_y' keys.\n",
    "    Each key's value should be a nested list structure representing data arrays.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Convert input JSON data to numpy arrays\n",
    "        train_X = np.array(data['train_X'])\n",
    "        train_y = np.array(data['train_y']).reshape(-1, 1)\n",
    "        validation_X = np.array(data['validation_X'])\n",
    "        validation_y = np.array(data['validation_y']).reshape(-1, 1)\n",
    "\n",
    "        # Load the saved model\n",
    "        model = load_model(MODEL_PATH)\n",
    "\n",
    "        # Callbacks for early stopping and learning rate reduction\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "        # Retrain the model\n",
    "        model.fit(\n",
    "            train_X, train_y,\n",
    "            validation_data=(validation_X, validation_y),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stopping, lr_scheduler],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Save the retrained model\n",
    "        model.save(MODEL_PATH)\n",
    "        return jsonify({\"message\": \"Model retrained and saved successfully.\"}), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_with_saved_model():\n",
    "    \"\"\"\n",
    "    Makes a prediction using the saved model.\n",
    "    Expects JSON input with 'input_data' key, which should be a nested list representing a single input sequence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        input_data = prepare_data(data['input_data'])  # Reshape input for prediction\n",
    "\n",
    "        # Load the saved model\n",
    "        model = load_model(MODEL_PATH)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(input_data)\n",
    "        predicted_value = prediction[0][0]\n",
    "\n",
    "        return jsonify({\"prediction\": float(predicted_value)}), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = '../models/final_model.h5'\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "def reshape_data(features: pd.DataFrame, n_timesteps: int = 7):\n",
    "    \"\"\"\n",
    "    Reshape the incoming features using a sliding window approach.\n",
    "\n",
    "    Parameters:\n",
    "    - features (pd.DataFrame): The input features.\n",
    "    - n_timesteps (int): The number of timesteps to use for reshaping.\n",
    "\n",
    "    Returns:\n",
    "    - reshaped_X (np.ndarray): Reshaped features.\n",
    "    \"\"\"\n",
    "    reshaped_X = np.array([\n",
    "        features.values[i:i + n_timesteps] \n",
    "        for i in range(len(features) - n_timesteps)\n",
    "    ])\n",
    "    return reshaped_X\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Endpoint to make predictions based on incoming data.\n",
    "    \"\"\"\n",
    "    # Get the JSON data from the request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Convert JSON data to DataFrame\n",
    "    features = pd.DataFrame(data)\n",
    "\n",
    "    # Reshape the data\n",
    "    n_timesteps = 7  # Adjust based on your model's requirements\n",
    "    reshaped_X = reshape_data(features, n_timesteps)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(reshaped_X)\n",
    "\n",
    "    # Return predictions as JSON\n",
    "    return jsonify(predictions.tolist())\n",
    "\n",
    "@app.route('/retrain', methods=['POST'])\n",
    "def retrain():\n",
    "    \"\"\"\n",
    "    Endpoint to retrain the model with new incoming data.\n",
    "    \"\"\"\n",
    "    # Get the JSON data from the request\n",
    "    data = request.get_json()\n",
    "    \n",
    "    # Convert JSON data to DataFrame\n",
    "    features = pd.DataFrame(data['features'])\n",
    "    target = pd.Series(data['target'])\n",
    "\n",
    "    # Reshape the incoming data\n",
    "    n_timesteps = 7  # Adjust based on your model's requirements\n",
    "    train_X, train_y = reshape_data(features, target)\n",
    "\n",
    "    # Retrain the model\n",
    "    global model  # Use the global model variable\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Save the retrained model\n",
    "    model.save(model_path)\n",
    "\n",
    "    return jsonify({\"message\": \"Model retrained successfully.\"})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
